{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "a = \"purple is the best city in the forest\".split()\n",
    "b = \"there is an art to getting your way and throwing bananas on to the street is not it\".split()\n",
    "c = \"it is not often you find soggy bananas on the street\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(word):\n",
    "    tf = []\n",
    "    count_n = 0\n",
    "    for sentence in [a, b, c]:\n",
    "        # calculate TF\n",
    "        t_count = len([x for x in sentence if word in sentence])\n",
    "        tf.append(t_count/len(sentence))\n",
    "        # count number of docs for IDF\n",
    "        count_n += 1 if word in sentence else 0\n",
    "    idf = np.log10(len([a, b, c]) / count_n)\n",
    "    return [round(_tf*idf, 2) for _tf in tf]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF a: 0.48\n",
      "TF-IDF b: 0.0\n",
      "TF-IDF c: 0.0\n"
     ]
    }
   ],
   "source": [
    "tfidf_a, tfidf_b, tfidf_c = tfidf('forest')\n",
    "print(f\"TF-IDF a: {tfidf_a}\\nTF-IDF b: {tfidf_b}\\nTF-IDF c: {tfidf_c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'find', 'not', 'it', 'and', 'throwing', 'there', 'often', 'forest', 'art', 'getting', 'way', 'city', 'an', 'the', 'street', 'you', 'your', 'soggy', 'in', 'purple', 'best', 'is', 'to', 'bananas', 'on'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vocab = set(a+b+c)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.48, 0.0, 0.0, 0.0, 0.48, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.48, 0.48, 0.48, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# initialize vectors\n",
    "vec_a = []\n",
    "vec_b = []\n",
    "vec_c = []\n",
    "\n",
    "for word in vocab:\n",
    "    tfidf_a, tfidf_b, tfidf_c = tfidf(word)\n",
    "    vec_a.append(tfidf_a)\n",
    "    vec_b.append(tfidf_b)\n",
    "    vec_c.append(tfidf_c)\n",
    "\n",
    "print(vec_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.18, 0.18, 0.48, 0.48, 0.48, 0.0, 0.0, 0.48, 0.48, 0.48, 0.0, 0.48, 0.0, 0.18, 0.0, 0.48, 0.0, 0.0, 0.0, 0.0, 0.0, 0.48, 0.18, 0.18]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(vec_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From there we have our TF-IDF vector. It’s worth noting that vocab sizes can easily be in the 20K+ range, \n",
    "# so the vectors produced using this method are incredibly sparse — which means we cannot encode any semantic meaning.\n",
    "\n",
    "# We can still use the TF-IDF vector documents to measure the distance between these"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "015cc5a99857e9fcd1ab0927880e4505b72089e0473c7b827a9368dc8509f23c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
